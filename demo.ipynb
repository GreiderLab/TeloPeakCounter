{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "import importlib\n",
    "import TeloFast5V2\n",
    "from TeloFast5V2 import *\n",
    "# from multiprocessing import Pool\n",
    "# !pip install pandarallel\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "importlib.reload(TeloFast5V2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './data/F63Fast5/SquigglePull.F63.batch0-1-2.tsv.gz'\n",
    "\n",
    "# Read the first line of the TSV file using csv.reader\n",
    "# with open(file_path, 'r', newline='') as tsv_file:\n",
    "#     tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "#     header = next(tsv_reader)\n",
    "\n",
    "# signal = header[2:]\n",
    "# signal = [float(i) for i in signal]\n",
    "\n",
    "# ****** Randomize the inputted table before grabbing subset, the first reads will be shorter than those read later on \n",
    "\n",
    "reducedDataDf = pd.read_csv(file_path, sep='\\t', compression=\"gzip\", error_bad_lines=False)\n",
    "\n",
    "reducedDataDf[\"signal\"] = reducedDataDf[\"signal\"].apply(lambda x: [float(i) for i in ast.literal_eval(x)])\n",
    "signalDf = reducedDataDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start processing\")\n",
    "dfDvsG = pd.read_csv(\"../Data/F63/reads.table_F63.TeloBP.prim.ClaI.NB50uq_alnScTh.20.txt\", sep=\"\\t\")\n",
    "print(\"Finished reading dfDvsG\")\n",
    "file_path = './data/F63Fast5/SquigglePull.F63.batch0-1-2.tsv/SquigglePull.data.1000SubSample.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the head of file_path\n",
    "with open(file_path, 'r', newline='') as tsv_file:\n",
    "    tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "    # grab the 7th line\n",
    "    for i in range(7):\n",
    "        header = next(tsv_reader)\n",
    "    print(header)\n",
    "    print(len(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r', newline='') as tsv_file:\n",
    "    chunk = csv.reader(tsv_file, delimiter='\\t')\n",
    "\n",
    "    print(\"Processing chunk\")\n",
    "    # for each row in chunk, convert signal column from string to list of floats\n",
    "    # for index, row in chunk.iterrows():\n",
    "    for row in chunk:\n",
    "        qnm = row[1]\n",
    "        print(qnm)\n",
    "        # signal = row[2:].tolist()\n",
    "        signal = row[2:]\n",
    "\n",
    "        # print(signal)\n",
    "        # remove nan and non numerics\n",
    "        signal = [float(x) for x in signal if isinstance(x, (int, float)) or (isinstance(x, str) and x != 'nan' and not x.endswith('.fast5') and \"-\" not in x)]\n",
    "        # signal = [x for x in signal if str(x) != 'nan']\n",
    "        # signal = [float(i) for i in signal]\n",
    "        chunkList.append([qnm, signal])\n",
    "    print(len(chunkList))\n",
    "    # break close file\n",
    "    tsv_file.close()\n",
    "\n",
    "print(\"done loading\")\n",
    "# chunkList = chunkList[0:100]\n",
    "\n",
    "# # chunkList = pd.read_csv(file_path, sep='\\t', error_bad_lines=False)\n",
    "\n",
    "print(len(chunkList))\n",
    "signalDf = pd.DataFrame(chunkList, columns=[\"qname\", \"signal\"])\n",
    "\n",
    "\n",
    "# chunk[\"signal\"] = chunk[\"signal\"].apply(lambda x: [float(i) for i in ast.literal_eval(x)])\n",
    "# signalDf = chunk\n",
    "# drop all columns in signalDf except qname and signal\n",
    "signalDf = signalDf[['qname', 'signal']] \n",
    "dvsGWithSignal = dfDvsG[dfDvsG[\"qname\"].isin(signalDf[\"qname\"])]    \n",
    "dvsGWithSignal[\"isGStrand\"] = dvsGWithSignal.apply(lambda row: isGStrand(row[\"chr\"][-1] ,row[\"strand\"]),axis=1)\n",
    "signalDf = signalDf.merge(dvsGWithSignal[['qname', 'isGStrand']], on='qname', how='left')\n",
    "\n",
    "pandarallel.initialize(progress_bar=True )\n",
    "\n",
    "signalDf[\"PeakCountBasedLengthGuppy\"] = signalDf.parallel_apply(getTeloCountLength,axis=1)\n",
    "\n",
    "# drop signal column\n",
    "saveDf = signalDf.drop(columns=[\"signal\"])\n",
    "# chunks.append(saveDf)\n",
    "\n",
    "# outputDf = pd.concat(chunks, axis=0)\n",
    "dvsGWithSignalShortened = signalDf\n",
    "saveDf.to_csv('output/F63.subset1000.peak.count.V3.1.tsv', sep='\\t')\n",
    "print(\"Finished processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfDvsG = pd.read_csv(\"./data/F61.F71.telomeres.for.Kayarash.txt\", sep=\"\\t\")\n",
    "dfDvsG = pd.read_csv(\"../Data/F63/reads.table_F63.TeloBP.prim.ClaI.NB50uq_alnScTh.20.txt\", sep=\"\\t\")\n",
    "\n",
    "# dfDvsG = pd.read_csv(\"./data/mappings/for.Ramin.Tel.diff.by.mapping.10000.txt\", sep=\" \")\n",
    "dfDvsG\n",
    "dvsGWithSignal = dfDvsG[dfDvsG[\"qname\"].isin(signalDf[\"qname\"])]    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
